{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cnn_pr00_New (1).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaimaaosman/ESP_Training/blob/master/Cnn_pr0ject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvnjx-fZZS5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install livelossplot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19QZgszFu2QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-B5X3PBZ7VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#importting the keras libirary and packages\n",
        "import pickle\n",
        "import gzip\n",
        "import os\n",
        "import random\n",
        "from livelossplot import PlotLossesKeras\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import  Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from google.colab import drive\n",
        "np.random.seed(np.random.randint(0, 40))\n",
        "#print number of version from Tensorflow is useing \n",
        "print('Tensorflow version:', tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpyI6RGNavrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''loads a saved pickle file that represents an object and returns that object'''\n",
        "def load(file_name):\n",
        "    # load the model\n",
        "    stream = gzip.open(file_name, \"rb\")\n",
        "    model = pickle.load(stream)\n",
        "    stream.close()\n",
        "    return model\n",
        "'''save model that is an object as file_name as a pickle file'''\n",
        "def save(file_name, model):\n",
        "    # save the model\n",
        "    stream = gzip.open(file_name, \"wb\")\n",
        "    pickle.dump(model, stream)\n",
        "    stream.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziTK0hOnqp-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocessing\n",
        "from google.colab import drive\n",
        "train_datagen = ImageDataGenerator(\n",
        "                #samplewise_std_normalization = True,\n",
        "                rescale = 1./255,\n",
        "                zoom_range=0.2,\n",
        "                validation_split = 0.30,\n",
        "                horizontal_flip = True,\n",
        "                vertical_flip = True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/drive/My Drive/dataset',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=98,\n",
        "        class_mode='categorical',\n",
        "        subset ='training')\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        '/content/drive/My Drive/dataset',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=98,\n",
        "        class_mode='categorical',\n",
        "        subset ='validation')\n",
        "\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/My Drive/dataset',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=100,\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upCuBT4wwIjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''The CNN Model'''\n",
        "\n",
        "# Initialising the CNN\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolution\n",
        "model.add(Conv2D(32,(5,5), padding='same', input_shape=(256, 256,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd Convolution layer\n",
        "model.add(Conv2D(64,(5,5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flattening\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC4C64TbJVTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''sheduling the learning rate and compiling the model'''\n",
        "\n",
        "initial_learning_rate = 0.005\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=5,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkcwiEwMOHap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Training the model'''\n",
        "\n",
        "#using early stopping to exit training if validation loss is not decreasing even after certain epochs  \n",
        "earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "\n",
        "#save the bast model with lower validation loss\n",
        "checkpoint = ModelCheckpoint(\"{epoch}th_epoch_model_weights.h5\", monitor='val_loss',\n",
        "                              save_weights_only=True, mode='min', verbose=1,  save_best_only=True)\n",
        "\n",
        "\n",
        "callbacks = [PlotLossesKeras()]#, reduce_lr]\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch= train_generator.n // 98,\n",
        "    epochs = 37,\n",
        "    validation_data= validation_generator,\n",
        "    validation_steps= validation_generator.n // 98,\n",
        "    callbacks=callbacks)\n",
        "\n",
        "\n",
        "'''Evaluating the model'''\n",
        "\n",
        "evaluate = model.evaluate_generator(test_generator, steps = test_generator.n // 100, verbose =1)\n",
        "classes = ['beach', 'desert', 'forest', 'industrial', 'mountain', 'sea ice']\n",
        "fig, axs = plt.subplots(5,6, figsize=(32,32))\n",
        "main_dir = '/content/drive/My Drive/dataset'\n",
        "for i in range(5) :\n",
        "    cnt = 0\n",
        "    for dir in os.listdir(main_dir) :\n",
        "        images = os.listdir(os.path.join(main_dir, dir))\n",
        "        j = random.randint(0, len(images))\n",
        "        image = Image.open(os.path.join(main_dir, dir, images[j]))\n",
        "        image = np.asarray(image, dtype = 'float64')\n",
        "        image  = image / 255.\n",
        "        y_pred = np.argmax(model.predict(np.asarray([image])), 1)\n",
        "        axs[i][cnt].imshow(image)\n",
        "        axs[i][cnt].set_title(\"prediction : {} , true class : {}\".format(classes[y_pred[0]], dir), fontsize = 16)\n",
        "        cnt = cnt + 1\n",
        "fig.tight_layout()\n",
        "    \n",
        "model.save_weights('mymodel_weights.h5')\n",
        "model.save('mymodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0sPjR7TfnQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('mymodel_weights.h5')\n",
        "\n",
        "evaluate = model.evaluate_generator(test_generator, steps = test_generator.n // 100, verbose =1)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}